# -*- coding: utf-8 -*-
"""comparison-of-classification-disease-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19CFYa4y_TTgwPT-qvRVZG-Uq-mr9Wztp

### **Modullerin Import Edilmesi** <a id="2"></a>

Proje içerisinde kullanılacak python modülleri import edilecektir.
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop,Nadam,Adadelta,Adam
from tensorflow.keras.layers import BatchNormalization,LeakyReLU
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
import seaborn as sns
import scipy.stats as stats
import sklearn
import os

"""### **Veri Temizliği ve Açıklayıcı Veri Analizi** <a id="3"></a>
<hr>
Öncelikle verilerimizde neye sahip olduğumuzu inceleyelim.
"""

data_raw = pd.read_csv("/content/cardio_train.csv", sep=";")
data_raw.head()

data_raw.info()

data_raw.drop("id",axis=1,inplace=True)

"""### **Tekrar Eden ve Eksik Değerleri Kontrol Etme** <a id="4"></a>

<hr>
    
Görselleştirme ve aykırı değer kontrollerinden önce, tekrar eden ve eksik değerleri ele almak çok önemlidir.
"""

print("There is {} duplicated values in data frame".format(data_raw.duplicated().sum()))

"""* Model eğitimine herhangi bir etkisi olmadığı için tekrar eden verileri bırakabiliriz.
* Öncelikle tekrar eden satırları gözle görelim.
"""

duplicated = data_raw[data_raw.duplicated(keep=False)]
duplicated = duplicated.sort_values(by=['age', "gender", "height"], ascending= False) 

duplicated.head(2)

data_raw.drop_duplicates(inplace=True)
print("There is {} duplicated values in data frame".format(data_raw.duplicated().sum()))

print("There is {} missing values in data frame".format(data_raw.isnull().sum().sum()))

"""Artık veri setimiz temiz.

### **Görselleştirme** <a id="5"></a>

<hr>
### Aykırı Değerlerin Tespiti <a id="6"></a>

Aykırı değerleri tespit etmek ve bunları ele almak doğruluk değerimizi arttırabilir.
"""

x = data_raw.copy(deep=True)
x.describe()

"""* "Yaş", "boy", "ağırlık", "ap_hi", "ap_lo" sütunları aykırı değerlere sahip olabilir.
* Onları aynı ölçekte karşılaştırmak için öncelikle standartlaştırmamız gerekiyor.

<hr>
#### Standart Skaler Fonksiyonum
"""

s_list = ["age", "height", "weight", "ap_hi", "ap_lo"]
def standartization(x):
    x_std = x.copy(deep=True)
    for column in s_list:
        x_std[column] = (x_std[column]-x_std[column].mean())/x_std[column].std()
    return x_std 
x_std=standartization(x)
x_std.head()

"""* Multi boxplot grafiğini kullanmak için verileri eritmemiz gerekir.

"""

x_melted = pd.melt(frame=x_std, id_vars="cardio", value_vars=s_list, var_name="features", value_name="value", col_level=None)
x_melted

plt.figure(figsize=(10,10))
sns.boxplot(x="features", y="value", hue="cardio", data=x_melted)
plt.xticks(rotation=90)

"""* Veri setinde bazı aykırı değerler vardır, ancak yukarıda görüldüğü gibi ap_hi ve ap_lo özelliklerinde alışılmadık bir aykırı değer vardır.
* ap_lo ve ap_hi özelliklerinin alt sınırını ve yüksek sınırını hesaplayalım
"""

ap_list = ["ap_hi", "ap_lo"]
boundary = pd.DataFrame(index=["lower_bound","upper_bound"])
for each in ap_list:
    Q1 = x[each].quantile(0.25)
    Q3 = x[each].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1- 1.5*IQR
    upper_bound = Q3 + 1.5*IQR
    boundary[each] = [lower_bound, upper_bound ]
boundary

"""* Hesapladığımız sınırları kullanarak aykırı değer verilerinin dizinini seçebiliriz.
* Normalde hem üst uç değerleri hem de alt uç değerleri analiz etmeliyiz, ancak bu durumda, son derece yüksek değerleri nedeniyle sadece üst uçlarla başa çıkmamız yeterli olacaktır.
"""

ap_hi_filter = (x["ap_hi"] > boundary["ap_hi"][1])
ap_lo_filter = (x["ap_lo"] > boundary["ap_lo"][1])                                                           
outlier_filter = (ap_hi_filter | ap_lo_filter)
x_outliers = x[outlier_filter]
x_outliers["cardio"].value_counts()

"""* ap_hi ve ap_lo aykırı değer verilerinin yüzde 83'ünde kardiyovasküler hastalık mevcuttur,
* ap_hi ve ap_lo yüksek tansiyonu simgelediğinden, yüksek hastalık oranı gerçek yaşamla tutarlıdır.
* Bu nedenle, tıbbi olarak imkansız verileri veri setinden çıkaralım.
* Ve bazı verileri düşürdüm çünkü yeterli veriye sahibiz (70000) aksi takdirde yeni değerler atayarak bunları işlemem gerekirdi.
"""

sns.countplot(x='cardio',data=x_outliers,linewidth=2,edgecolor=sns.color_palette("dark", 1))

"""* Bir kişinin sistolik basıncı (ap_hi) 180'i aşarsa veya diyastolik basınç (ap_lo) 120'yi aşarsa, bu durum acil tıbbi müdahale gerektiren bir aşamadır.

[![blood.jpg](https://i.postimg.cc/bwzm4sq5/blood.jpg)](https://postimg.cc/2VKdZjbd)

* NCBI NLM'de doktorlar tarafından yayınlanan bir çalışmada maksimum kan basıncı 370/360 mm Hg olarak kaydedilmiş. Bu çalışma 10 erkek sporcuda radiyal arter kateterizasyonu ile kan basıncı kaydedilerek yapılmış.
* Böylece, veri kaybı korkusu olmadan ap_hi aykırı değerleri 250'nin üzerine ve ap_lo aykırı değerleri 200'ün üzerine düşürebiliriz.
"""

out_filter = ((x["ap_hi"]>250) | (x["ap_lo"]>200) )
print("There is {} outlier".format(x[out_filter]["cardio"].count()))

x = x[~out_filter]

corr = x.corr()
f, ax = plt.subplots(figsize = (15,15))
sns.heatmap(corr, annot=True, fmt=".3f", linewidths=0.5, ax=ax)

"""* Korelasyon haritasından kolaylıkla görebiliriz; kolesterol, kan basıncı (her ikisi de ap_hi ve yaklaşık olarak) ve yaşın kardiyovasküler hastalıklarla güçlü bir ilişkisi vardır.
* Glukojen ve kolesterol de aralarında güçlü bir ilişkiye sahiptir.

### **Özellik Mühendisliği** <a id="7"></a>

<hr>
### Vücut Kitle İndeksi Özelliği
  
Boy ve ağırlık, kardiyo özelliği ile ilintisiz görünüyor, ancak Vücut Kitle İndeksi modelimizi eğitmek için yardımcı olabilir.
"""

def bmi_calc(w, h):
    return w/(h**2)

x["bmi"] = x["weight"]/ (x["height"]/100)**2

x.head()

"""* Hastaların cinsiyetlerini tespit etmek"""

a = x[x["gender"]==1]["height"].mean()
b = x[x["gender"]==2]["height"].mean()
if a > b:
    gender = "male"
    gender2 = "female"
else:
    gender = "female"
    gender2 = "male"
print("Gender:1  "+ gender +" & Gender:2  " + gender2)

"""* Kadınlar, özellikle 65 yaşından sonra sigara, yüksek tansiyon ve yüksek kolesterol gibi erkeklerle aynı risk faktörlerinin çoğuna sahiptir.
* Dolayısıyla onları 1 ve 2 olarak sınıflandırmamalıyız çünkü 2 sayısal olarak her zaman 1'den büyüktür, model bunu hesaba katar ve bir hastalığa sahip erkeklere daha büyük bir oran verir.
* Diğer kategorik kodu tek bir etkin kodlamaya değiştirmedik çünkü bunlar gerçekten hiyerarşik boyutu ifade ediyorlar
* Veri setinin açıklamasından bir örnek: Kolesterol | 1: normal, 2: normalin üstünde, 3: normalin çok üstünde

"""

x["gender"] = x["gender"] % 2

from sklearn.preprocessing import StandardScaler
x_std = standartization(x)

data = pd.melt(x_std,id_vars="cardio",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="gender", y="bmi", hue="cardio", data=x,split=True, inner="quart")
plt.xticks(rotation=90)

"""Keman grafiğini yorumlarsak, hastaların bmi dağılımının medyan ve çeyrekleri hasta olmayanlara göre biraz daha yüksektir.

### **Model Seçimi** <a id="8"></a>
<hr>
### Eğitim ve Test Setlerinin Hazırlanması <a id="9"></a>

Aykırı değeri tespit etmek ve bunları ele almak doğruluk puanımızı artırabilir.

Bir model oluşturmak için öncelikle verilerimizi eğitim ve test setine ayıracağız.
"""

y = x["cardio"]
y.shape

x.drop("cardio", axis=1,inplace=True)
x.head()

from sklearn.model_selection import train_test_split
x_train,x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)

"""### Veri Normalleştirme <a id="10"></a>

<hr>
"""

from sklearn.preprocessing import normalize
x_train = normalize(x_train)
x_test = normalize(x_test)
x = normalize(x)

"""### Model Karşılaştırması <a id="11"></a>

<hr>
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

dec = DecisionTreeClassifier()
ran = RandomForestClassifier(n_estimators=100)
knn = KNeighborsClassifier(n_neighbors=100)
svm = SVC(random_state=1)
naive = GaussianNB()

models = {"Decision tree" : dec,
          "Random forest" : ran,
          "KNN" : knn,
          "SVM" : svm,
          "Naive bayes" : naive}
scores= { }

for key, value in models.items():    
    model = value
    model.fit(x_train, y_train)
    scores[key] = model.score(x_test, y_test)

scores_frame = pd.DataFrame(scores, index=["Accuracy Score"]).T
scores_frame.sort_values(by=["Accuracy Score"], axis=0 ,ascending=False, inplace=True)
scores_frame

plt.figure(figsize=(5,5))
sns.barplot(x=scores_frame.index,y=scores_frame["Accuracy Score"])
plt.xticks(rotation=45)

"""* Görünüşe göre KNN ve Random Forest algoritmaları diğerlerinden çok daha ileride.
* Öyleyse bu algoritmalara odaklanalım
 
### K Fold Cross Validation <a id="12"></a>

<hr>
    
* K Fold Cross Validation ile, ana eğitim setimizden farklı "K" mini eğitim setlerinden sonuçlar elde ediyoruz.
* Daha sonra bu sonuçların ortalamasını gerçek sonuç olarak seçiyoruz.
* Sonuçta, K sonucunun standart sapmasını alarak verilerin tutarlı olup olmadığını inceleyebiliriz.
"""

from sklearn.model_selection import cross_val_score
accuracies_random_forest = cross_val_score(estimator=ran, X=x_train, y=y_train, cv=10)
accuracies_knn = cross_val_score(estimator=knn, X=x_train, y=y_train, cv=10)

print("Random Forest Average accuracy: ", accuracies_random_forest.mean())
print("Random Forest Standart Deviation: ", accuracies_random_forest.std())
print("KNN Average accuracy: ", accuracies_knn.mean())
print("KNN Standart Deviation: ", accuracies_knn.std())

"""Standart sapma değeri, tutarlı sonuçlar elde ettiğimizi gösterir.

### Izgara Araması (Grid Search) <a id="13"></a>

<hr>
 Modelimiz için en iyi Rastgele Orman Algoritmaları "n_estimators" hiperparametre değerini bulmak için ızgara arama algoritmasını kullanalım.
"""

from sklearn.model_selection import GridSearchCV

grid = {"n_estimators" : np.arange(10,150,10)}

ran_cv = GridSearchCV(ran, grid, cv=3) # GridSearchCV
ran_cv.fit(x_train,y_train)# Fit

print("Tuned hyperparameter n_estimators: {}".format(ran_cv.best_params_)) 
print("Best score: {}".format(ran_cv.best_score_))

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(solver="liblinear", max_iter=200)
grid = {"penalty" : ["l1", "l2"],
         "C" : np.arange(60,80,2)} # (60,62,64 ... 78)
log_reg_cv = GridSearchCV(log_reg, grid, cv=3)
log_reg_cv.fit(x_train, y_train)


print("Tuned hyperparameter n_estimators: {}".format(log_reg_cv.best_params_)) 
print("Best score: {}".format(log_reg_cv.best_score_))

"""* Elde ettiğimiz sonuçların en iyisi bu
* En iyi modelimizi değerlendirelim
"""



"""### Model Değerlendirmesi <a id="14"></a>

<hr>
### Test Seti Doğruluk Puanı <a id="15"></a>    

* Artık varsayılanlardan daha iyi hiper parametrelere sahip modelimizi seçtik.
* Test setimiz ile modeli değerlendirebiliriz artıkç
"""

logreg_best = LogisticRegression(C=74, penalty="l1", solver="liblinear")
logreg_best.fit(x_train, y_train)
print("Test accuracy: ",logreg_best.score(x_test, y_test))

"""Modelimizin eğitimde kullanılmayan test setini tahmin etmedeki nihai başarısı 0,72'dir.

### Karışıklık Matrisi (Confusion Matrix) <a id="16"></a>

<hr>
"""

y_true = y_test
y_pred = logreg_best.predict(x_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, y_pred)

f, ax = plt.subplots(figsize=(5,5))
sns.heatmap(cm,fmt=".0f", annot=True,linewidths=0.2, linecolor="purple", ax=ax)
plt.xlabel("Predicted")
plt.ylabel("Grand Truth")
plt.show()

"""### F1 Score <a id="17"></a>

<hr>
"""

TN = cm[0,0]
TP = cm[1,1]
FN = cm[1,0]
FP = cm[0,1]
Precision = TP/(TP+FP)
Recall = TP/(TP+FN)
F1_Score = 2*(Recall * Precision) / (Recall + Precision)
pd.DataFrame([[Precision, Recall, F1_Score]],columns=["Precision", "Recall", "F1 Score"], index=["Results"])

"""1. Yüksek hassasiyet, düşük false positive (yanlış pozitif) oranıyla ilgilidir.
1. Yüksek hatırlama, düşük false negative (yanlış negatif) oranıyla ilgilidir.

## YSA Yaklaşımı <a id="18"></a>

<hr>
"""

x.shape

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(6, input_dim=12, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

optimizer = RMSprop(learning_rate=0.002)
model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=optimizer)

learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau( 
    monitor='val_loss',   
    factor=0.1,       
    patience=50,        
    verbose=1,         
    mode="auto",                    
    min_delta=0.0001,  
    cooldown=0,        
    min_lr=0.00001    
    )

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=400, restore_best_weights=True)
history = model.fit(x=x_train, y=y_train.values,
                    batch_size=1024, epochs=1500,
                    verbose=0,validation_data=(x_test,y_test.values),
                    callbacks=[learning_rate_reduction, es],
                    shuffle=True)

model.evaluate(x_test, y_test.values, verbose=2)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()